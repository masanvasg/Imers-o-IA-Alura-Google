{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masanvasg/Imers-o-IA-Alura-Google/blob/main/C%C3%B3pia_de_Andrew_Ng_Suite_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkGpI_1qoq9S"
      },
      "source": [
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTK4gQ9nhwHHaSXMHpeggWg7twwMCgb877smkRmtkmDeDoGF9Z6&usqp=CAU)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color=blue>AI Suite\n",
        "![](https://www.tomsawyer.com/hubfs/Graph%20Platform/2023.12.28.0.K5ColorAnimated.gif)\n",
        "\n",
        "É um wrapper (compilador) leve para fornecer uma interface unificada entre provedores de LLM."
      ],
      "metadata": {
        "id": "hZq_yZRcbxdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/andrewyng/aisuite"
      ],
      "metadata": {
        "id": "DOKDxsDv2u7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuite[all]"
      ],
      "metadata": {
        "id": "1mt8kgFHXMvv",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5560ce1d-4331-4134-b081-7d8d430d452c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cohere<6.0.0,>=5.12.0 (from aisuite[all])\n",
            "  Downloading cohere-5.13.11-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from aisuite[all])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.59.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.3)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.11-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, httpx, groq, aisuite, cohere, anthropic\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed aisuite-0.1.7 anthropic-0.30.1 cohere-5.13.11 fastavro-1.10.0 groq-0.9.0 httpx-0.27.2 httpx-sse-0.4.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Pretty Printing Function\n",
        "Nesta seção, definimos uma função de impressão bonita personalizada que melhora a legibilidade das estruturas de dados quando impressas. Esta função utiliza o módulo pprint integrado do Python, permitindo aos usuários especificar uma largura personalizada para formatação de saída."
      ],
      "metadata": {
        "id": "KwFlLByRbWKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query\n"
      ],
      "metadata": {
        "id": "-Wf7j6abbQmw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "Aqui definiremos com segurança nossas chaves de API como variáveis ​​de ambiente. Isso é útil porque não queremos codificar informações confidenciais (como chaves de API) diretamente em nosso código. Ao usar variáveis ​​de ambiente, podemos manter nossas credenciais seguras e ao mesmo tempo permitir que nosso programa as acesse. Normalmente usaríamos um arquivo .env para armazenar nossas senhas em nossos ambientes, mas como trabalharemos no Colab, faremos as coisas um pouco diferentes."
      ],
      "metadata": {
        "id": "Cce1aLBvctaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsK7GrHyV-c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Simple Chat Interaction with an AI Language Model\n",
        "Este código inicia uma interação de chat com um modelo de linguagem (especificamente o LLaMA 3.2 da Groq), onde o modelo responde à entrada do usuário. Usamos a biblioteca aisuite para nos comunicarmos com o modelo e recuperar a resposta."
      ],
      "metadata": {
        "id": "m2mhu-VbSWfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "mBEOEq99eGjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61161a5f-16d7-4af2-e1d2-6ddd1b0aa051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how can I assist you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definindo uma função para interagir com o modelo de linguagem\n",
        "\n",
        "Esta função, ask, agiliza o processo de envio de uma mensagem do usuário para um modelo de linguagem e recuperação de uma resposta. Ele encapsula a lógica necessária para configurar a conversa e pode ser reutilizado em todo o notebook para diferentes consultas. Não preservará nenhum histórico ou qualquer conversa contínua.  \n",
        "\n"
      ],
      "metadata": {
        "id": "YJSahowjiJBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"groq:llama-3.2-3b-preview\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "n8DK8_RqqXFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Fala, beleza?. Qual é a capital de Fiji?\")"
      ],
      "metadata": {
        "id": "FGcqY4lBjtFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a49da317-c43a-479b-a4b5-99ad378c000a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá!\\n\\nSim, fala bem! Você está perguntando sobre o Fijian, mas está falando em inglês!\\n\\nA capital do Fijian é Suva!\\n\\nSuva é a maior cidade do Fijian e é localizada na ilha principal de Viti Levu. É um importante centro administrativo, econômico e cultural do país.\\n\\nEspero que isso tenha ajudado! Queria mais alguma coisa?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O verdadeiro valor do AI Suite é a capacidade de executar uma variedade de modelos diferentes.  Vamos primeiro configurar uma coleção de diferentes chaves de API que podemos testar."
      ],
      "metadata": {
        "id": "wpeW6Pj6j_6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI API key: ')\n",
        "os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your ANTHROPIC API key: ')\n",
        "os.environ['FIREWORKS_API_KEY'] = getpass('Enter your FIREWORKS API key: ')\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')\n",
        "os.environ['HUGGINGFACE_API_KEY'] = getpass('Enter your HUGGINGFACE API key: ')\n",
        "os.environ['GOOGLE_API_KEY'] = getpass('Enter your GOOGLE API key: ')\n",
        "os.environ['Deepseek_API_KEY] = getpass('Enter your Deepseek API key: ')\n",
        "os.environ['perplexit_api_key'] = getpass('Enter your perplexit API key: ')\n",
        "\n"
      ],
      "metadata": {
        "id": "9_kJlkGfj_NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0683eea-1cdf-403f-e8fd-d856c23b256a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI API key: ··········\n",
            "Enter your ANTHROPIC API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confirme se cada modelo está usando um provedor diferente\n"
      ],
      "metadata": {
        "id": "mfPtlJlbTY6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(ask(\"Quem é seu criador?\"))\n",
        "#print(ask('Quem é seu criador?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
        "print(ask('Quem é seu criadorr?', model='openai:gpt-4o'))\n",
        "\n"
      ],
      "metadata": {
        "id": "iHVESCGJuWWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4eee41-4707-4846-ec9d-6265d9f64862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu fui desenvolvido pela OpenAI, uma organização de pesquisa em inteligência artificial. A OpenAI é composta por uma equipe de pesquisadores e engenheiros que colaboram para criar tecnologias de IA avançadas. Se você tiver mais perguntas ou precisar de alguma informação, estou aqui para ajudar!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask('Quem é seu criadorr?', model='groq:llama-3.2-3b-preview'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IVijETEpnhi",
        "outputId": "f088ceea-541f-4d82-cfc0-3317305a7972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sou um modelo de linguagem conhecido como Llama. Llama significa \"Meta AI de modelo de linguagem grande\". \n",
            "\n",
            "Fui desenvolvido pela Meta, uma empresa tecnológica que foca em tecnologias de Inteligência Artificial, Redes Sociais e Outras Productos para Apoiar as Comunidades.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consultando vários modelos de IA para uma pergunta comum\n",
        "Nesta seção, consultaremos diversas versões diferentes do modelo de linguagem LLaMA para obter respostas variadas para a mesma pergunta. Essa abordagem nos permite comparar como diferentes modelos lidam com o mesmo prompt, fornecendo insights sobre seu desempenho e estilo."
      ],
      "metadata": {
        "id": "BWBL4D2H2B_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = [\n",
        "    'llama-3.1-8b-instant',\n",
        "    'llama-3.2-1b-preview',\n",
        "    'llama-3.2-3b-preview',\n",
        "    'llama3-70b-8192',\n",
        "    'llama3-8b-8192'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each model\n",
        "ret = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    ret.append(ask('Escreva uma breve explicação num parágrafo sobre as origens da IA?', model=f'groq:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(models[idx] + ': \\n ' + x + ' ')"
      ],
      "metadata": {
        "id": "E_gg-sgYuoOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1e1d30-2afd-4dac-9cdd-76ec918dd83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('llama-3.1-8b-instant: \\n'\n",
            " ' As origens da Inteligência Artificial (IA) remontam a finais do século XIX, '\n",
            " 'quando cientistas e filósofos começaram a se perguntar sobre a possibilidade '\n",
            " 'de máquinas pensar de forma semelhante ao ser humano. No entanto, foi com a '\n",
            " 'publicação do livro \"A Máquina do Mundo\" do filósofo e matemático '\n",
            " 'Christopher Cole, originalmente escrito na língua inglesa como \"A Paper '\n",
            " 'Machine a Universal Transformation Machine\" de 1936,  em 1950 até que o '\n",
            " 'conceito de IA como o pensamento conhecido começou a tomar forma. No '\n",
            " 'entanto, foi o matemático e informático Alan Turing quem verdadeiramente '\n",
            " 'popularizou o conceito de máquina pensante em seu artigo de 1950 \"Computing '\n",
            " 'Machinery and Intelligence\", onde apresentou a famosa questão da máquina do '\n",
            " 'Turing. Desde então, a IA tem evolvido rapidamente, com a criação de '\n",
            " 'algoritmos, redes neurais e outras técnicas que permitem que as máquinas '\n",
            " 'sejam capazes de aprender, raciocinar e resolver problemas de forma '\n",
            " 'crescentemente sofisticada. ')\n",
            "('llama-3.2-1b-preview: \\n'\n",
            " ' As origens da Inteligência Artificial (IA) têm uma história complexa e '\n",
            " 'multifacetada, que remonta a inícios dos séculos XIX e XX. O termo '\n",
            " '\"Inteligência Artificial\" foi coined pelo austríaco Wilhelm von Osterhteg am '\n",
            " 'Innício do século XX e foi amplamente adotado nos anos 1950 e 1960 pelos '\n",
            " 'матemáticos e cientistas da época. A sua ideia nasceu da combinação de '\n",
            " 'teorias da física computacional e da computação analítica, que permitiram '\n",
            " 'estimular o desenvolvimento de sistemas que podiam realizar tarefas, como '\n",
            " 'resolução de problemas matemáticos e imitação de comportamento humano. No '\n",
            " 'entanto, foi os matemáticos logários e na física teórica, James McClellan e '\n",
            " 'Alan Turing, através de seus esforços em regras em máquina computacional e '\n",
            " 'decodificação de códigos, que começaram a teclarar a forma como as IA de '\n",
            " 'agora são aplicadas em diversas áreas e de forma complexa. ')\n",
            "('llama-3.2-3b-preview: \\n'\n",
            " ' A Inteligência Artificial (IA) tem suas origens na segunda metade do século '\n",
            " 'XX, quando os cientistas começaram a explorar a ideia de criar sistemas '\n",
            " 'capazes de aprender e se adaptar como seres humanos. Um dos principais '\n",
            " 'impulsionadores desse movimento foi o matemático e computadorista Alan '\n",
            " 'Turing, que propôs em 1950 o Teste Turing - uma medida para determinar se um '\n",
            " 'sistema computacional era capaz de imitar o pensamento humano. Nos anos '\n",
            " 'seguintes, os cientistas trabalharam no desenvolvimento de sistemas como o '\n",
            " 'ELIZA, uma linguagem de programação que simulava uma conversa, e o PROLOG, '\n",
            " 'um sistema de tomadas de decisões que usava lógica e regras para resolver '\n",
            " 'problemas. Esses trabalhos marcou o início de uma nova era na computação, '\n",
            " 'onde a computação não era apenas uma tarefa definida de entrada saída, mas '\n",
            " 'também uma capacidade de aprender, inferir e crescer ao longo do tempo. ')\n",
            "('llama3-70b-8192: \\n'\n",
            " ' Aqui vai uma breve explicação sobre as origens da Inteligência Artificial '\n",
            " '(IA):\\n'\n",
            " '\\n'\n",
            " 'As origens da Inteligência Artificial (IA) remontam à antiguidade, com '\n",
            " 'filósofos como Aristóteles e Platão que especulavam sobre a possibilidade de '\n",
            " 'criar máquinas que pensassem e aprendessem como seres humanos. No entanto, a '\n",
            " 'IA como conhecemos hoje começou a tomar forma no século XX, com o trabalho '\n",
            " 'de pioneiros como Alan Turing, Marvin Minsky e John McCarthy. Em 1950, '\n",
            " 'Turing propôs o \"Teste de Turing\", um método para avaliar se uma máquina era '\n",
            " 'capaz de pensar como um ser humano. Nos anos 1950 e 1960, Minsky e McCarthy '\n",
            " 'desenvolveram os primeiros programas de computador capazes de aprender e '\n",
            " 'raciocinar. Desde então, a IA tem evoluído rapidamente, passando por avanços '\n",
            " 'em áreas como machine learning, redes neurais e processamento de linguagem '\n",
            " 'natural, tornando-se hoje uma tecnologia essencial em diversas áreas, desde '\n",
            " 'a saúde até a indústria e comércio. ')\n",
            "('llama3-8b-8192: \\n'\n",
            " ' A Inteligência Artificial (IA) tem uma história longa e complexa que '\n",
            " 'remonta ao século XX. As origens da IA podem ser traçadas de volta a 1950, '\n",
            " 'quando o computador universal, idealizado por Alan Turing, foi desenvolvido '\n",
            " 'pela primeira vez. Turing publicou um artigo chamado \"Computing Machinery '\n",
            " 'and Intelligence\", que propunha um teste para avaliar a inteligência '\n",
            " 'artificial: o teste Turing, também conhecido como \"oHomem ou Máquina\". Isso '\n",
            " 'inspirou muitos cientistas e engenheiros a desenvolver programas que '\n",
            " 'pudessem imitar a inteligência humana, levando ao surgimento de áreas como a '\n",
            " 'Inteligência Artificial, a Ciência da Computação e a Robótica. ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consultando diferentes provedores de IA para uma pergunta comum\n",
        "Nesta seção, consultaremos vários modelos de IA de diferentes fornecedores para obter respostas variadas à mesma pergunta sobre as origens da IA. Esta comparação nos permite observar como diferentes modelos de diferentes arquiteturas respondem ao mesmo prompt."
      ],
      "metadata": {
        "id": "Z8pnJPdD2NL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of AI model providers to query\n",
        "providers = [\n",
        "    'groq:llama3-70b-8192',\n",
        "    'openai:gpt-4o',\n",
        "    'groq:llama-3.2-1b-preview'\n",
        "   # 'anthropic:claude-3-5-sonnet-20240620'#\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each provider\n",
        "ret = []\n",
        "\n",
        "# Loop through each provider and get a response for the specified question\n",
        "for x in providers:\n",
        "    ret.append(ask('Escreva uma breve explicação de uma frase sobre as origens da IA?', model=x))\n",
        "\n",
        "# Print the provider's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j4TqhC5J1YIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ef012c-7871-47ee-9657-f4945022f5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('groq:llama3-70b-8192: \\n'\n",
            " 'Here is a brief explanation of a phrase about the origins of Artificial '\n",
            " 'Intelligence (AI):\\n'\n",
            " '\\n'\n",
            " '**Phrase:** \"The Dartmouth Summer Research Project on Artificial '\n",
            " 'Intelligence\"\\n'\n",
            " '\\n'\n",
            " '**Explanation:** This phrase refers to a pivotal event in the history of '\n",
            " 'Artificial Intelligence (AI). In 1956, computer scientist John McCarthy, '\n",
            " 'along with Marvin Minsky, Nathaniel Rochester, and Claude Shannon, organized '\n",
            " 'a summer research project at Dartmouth College to explore the possibilities '\n",
            " 'of creating machines that could simulate human intelligence. This meeting is '\n",
            " 'often considered the birthplace of Artificial Intelligence as a field of '\n",
            " 'research, marking the beginning of a journey to develop machines that can '\n",
            " \"think, learn, and act like humans. The project's goals were ambitious, \"\n",
            " 'including creating machines that could reason, solve problems, and learn '\n",
            " 'from experience. The ideas and concepts developed during this project laid '\n",
            " 'the foundation for the development of AI in the decades that followed. \\n'\n",
            " '\\n')\n",
            "('openai:gpt-4o: \\n'\n",
            " 'A inteligência artificial (IA) tem suas origens nos avanços da ciência da '\n",
            " 'computação e da matemática do século XX, especialmente com a formalização da '\n",
            " 'lógica e o desenvolvimento de algoritmos computacionais por pioneiros como '\n",
            " 'Alan Turing. \\n'\n",
            " '\\n')\n",
            "('groq:llama-3.2-1b-preview: \\n'\n",
            " 'A Física Analítica como precursor importante da Inteligência Artificial teve '\n",
            " 'seu início nos anos 19 e 20 de novecentos, ainda à altura da descoberta das '\n",
            " 'leis de Browniano e Planck. \\n'\n",
            " '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gerando e avaliando perguntas com modelos de IA\n",
        "Nesta seção, geraremos perguntas aleatoriamente usando um modelo de linguagem e, em seguida, faremos com que outros dois modelos forneçam respostas a essas perguntas. O usuário avaliará então qual resposta é melhor, permitindo uma análise comparativa das respostas de diferentes modelos."
      ],
      "metadata": {
        "id": "OgPCC0y_U4WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Initialize a list to store the best responses\n",
        "best = []\n",
        "\n",
        "# Loop to generate and evaluate questions\n",
        "for _ in range(20):\n",
        "    # Shuffle the providers list to randomly select models for each iteration\n",
        "    random.shuffle(providers)\n",
        "\n",
        "    # Generate a question using the first provider\n",
        "    question = ask('Por favor, gere uma pergunta curta que seja adequada para fazer um LLM.', model=providers[0])\n",
        "\n",
        "    # Get answers from the second and third providers\n",
        "    answer_1 = ask('Por favor, dê uma resposta curta a esta pergunta: ' + question, model=providers[1])\n",
        "    answer_2 = ask('Por favor, dê uma resposta curta a esta pergunta: ' + question, model=providers[2])\n",
        "\n",
        "    # Print the generated question and the two answers\n",
        "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
        "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
        "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
        "\n",
        "    # Store the provider names and the user's choice of the best answer\n",
        "    best.append(str(providers) + ', ' + input(\"Qual é melhor 1 ou 2. 3 se indistinguível: \"))"
      ],
      "metadata": {
        "id": "fMx-TfLk09ft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82a98a70-9091-4c33-b965-e39a8c7c4245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Original text:\\n'\n",
            " '  Que ideia!\\n'\n",
            " '\\n'\n",
            " 'Aqui está uma pergunta curta para um Large Language Model (LLM):\\n'\n",
            " '\\n'\n",
            " '\"Descreva um tipo de escrita ou arte que seja considerada \\'barroco\\' e '\n",
            " 'possa ser encontrada em museus ou galerias de arte.\"\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  A arte barroca é caracterizada por sua grandiosidade, detalhamento '\n",
            " 'exuberante e expressividade emocional. Na pintura, por exemplo, isso se '\n",
            " 'traduz em composições dramáticas com contrastes de luz e sombra, como nas '\n",
            " 'obras de Caravaggio. Na escultura, artistas como Bernini criaram peças '\n",
            " 'dinâmicas e detalhadas que evocam movimento e emoção, muitas das quais podem '\n",
            " 'ser encontradas em museus e igrejas por toda a Europa.\\n'\n",
            " '\\n')\n",
            "'Option 2 text:\\n  Que ideia!\\n\\nA resposta é: Iluminura!\\n\\n'\n",
            "Qual é melhor 1 ou 2. 3 se indistinguível: 1\n",
            "('Original text:\\n'\n",
            " '  Claro! Aqui está uma pergunta curta e adequada:\\n'\n",
            " '\\n'\n",
            " '\"Qual é o conceito de overfitting em modelos de aprendizado de máquina?\"\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  O overfitting em modelos de aprendizado de máquina ocorre quando um modelo '\n",
            " 'se ajusta muito bem aos dados de treinamento, mas não generaliza bem para '\n",
            " 'novos dados, apresentando um desempenho ruim em dados que não foram vistos '\n",
            " 'durante o treinamento. Isso significa que o modelo se tornou muito '\n",
            " 'especializado nos dados de treinamento e perdeu a capacidade de aprender '\n",
            " 'conceitos gerais que possam ser aplicados a outros dados.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  O conceito de overfitting em modelos de aprendizado de máquina ocorre '\n",
            " 'quando um modelo se adapta muito bem a um conjunto de dados particular, mas '\n",
            " 'mal a um conjunto de dados mais amplio.\\n'\n",
            " '\\n')\n",
            "Qual é melhor 1 ou 2. 3 se indistinguível: 3\n",
            "('Original text:\\n'\n",
            " '  Aqui vai uma pergunta curta que deve ser abordada em uma resposta completa '\n",
            " 'e informada:\\n'\n",
            " '\\n'\n",
            " 'Qual é a definição do conceito em questão?\\n'\n",
            " '\\n'\n",
            " 'Este tipo de pergunta é adequada porque permite que o LLM aborde um tópico '\n",
            " 'específico, o que pode ajudar a responder com precisão e relevantes.\\n'\n",
            " '\\n'\n",
            " 'Aqui está uma resposta possível:\\n'\n",
            " '\\n'\n",
            " '\"A definição do conceito em questão é uma pergunta complexa que envolve a '\n",
            " 'análise de um tema ou área, geralmente envolvendo conceitos lógicos e '\n",
            " 'abstratos. Em resumo, a definição requer uma análise cuidadosa e estruturada '\n",
            " 'para entender o conceito e sua relação com outros conceitos e ideias.\"\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  Sim, uma resposta curta e precisa para a pergunta \"Qual é a definição do '\n",
            " 'conceito em questão?\" poderia ser:\\n'\n",
            " '\\n'\n",
            " '\"A definição do conceito em questão é uma explicação clara e precisa do '\n",
            " 'significado e do escopo de um termo ou ideia, estabelecendo seus limites e '\n",
            " 'relações com outros conceitos.\"\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  A definição de um conceito geralmente envolve identificar suas '\n",
            " 'características essenciais e formais de maneira clara e concisa. Isso requer '\n",
            " 'uma compreensão aprofundada do tema, incluindo suas implicações e conexões '\n",
            " 'com outros conceitos e ideias. A análise cuidadosa é crucial para capturar a '\n",
            " 'essência do conceito e diferenciá-lo de outros relacionados.\\n'\n",
            " '\\n')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-706a2f68c9f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store the provider names and the user's choice of the best answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Qual é melhor 1 ou 2. 3 se indistinguível: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - QnA com PDF online\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nRVgX4dT72eD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlEEy5df8Vwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906678a0-e9a6-4e54-bb81-d095f3f00ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cohere<6.0.0,>=5.12.0 (from aisuite[all])\n",
            "  Downloading cohere-5.13.11-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from aisuite[all])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.59.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.11-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, python-dotenv, PyMuPDF, httpx-sse, fastavro, httpx, groq, aisuite, cohere, anthropic\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed PyMuPDF-1.25.2 aisuite-0.1.7 anthropic-0.30.1 cohere-5.13.11 fastavro-1.10.0 groq-0.9.0 httpx-0.27.2 httpx-sse-0.4.0 python-dotenv-1.0.1 types-requests-2.32.0.20241016\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF requests python-dotenv aisuite[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnYbfhIg8Vwl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "sys.path.append('../aisuite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy5gZyoO8Vwm"
      },
      "outputs": [],
      "source": [
        "import aisuite as ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlBRLGtA8Vwm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def configure_environment(additional_env_vars=None):\n",
        "    \"\"\"\n",
        "    Load environment variables from .env file and apply any additional variables.\n",
        "    :param additional_env_vars: A dictionary of additional environment variables to apply.\n",
        "    \"\"\"\n",
        "    # Load from .env file if available\n",
        "    load_dotenv(find_dotenv())\n",
        "\n",
        "    # Apply additional environment variables\n",
        "    if additional_env_vars:\n",
        "        for key, value in additional_env_vars.items():\n",
        "            os.environ[key] = value\n",
        "\n",
        "# Define additional API keys and credentials\n",
        "additional_keys = {}\n",
        "\n",
        "# Configure environment\n",
        "configure_environment(additional_env_vars=additional_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcEcMrm88Vwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7c3b35-2a43-47ca-80ae-7d787105c215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and extracted text from pdf.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import fitz\n",
        "from io import BytesIO\n",
        "\n",
        "# Link to paper in pdf format on the cost of avocados.\n",
        "pdf_path = \"https://arxiv.org/pdf/2501.09166\"\n",
        "pdf_text = \"\"\n",
        "# Download PDF and load it into memory\n",
        "response = requests.get(pdf_path)\n",
        "if response.status_code == 200:\n",
        "    pdf_data = BytesIO(response.content)  # Load PDF data into BytesIO\n",
        "    # Open PDF from memory using fitz\n",
        "    with fitz.open(stream=pdf_data, filetype=\"pdf\") as pdf:\n",
        "        text = \"\"\n",
        "        for page_num in range(pdf.page_count):\n",
        "            page = pdf[page_num]\n",
        "            pdf_text += page.get_text(\"text\")  # Extract text\n",
        "            pdf_text += \"\\n\" + \"=\"*50 + \"\\n\"  # Separator for each page\n",
        "    print(\"Downloaded and extracted text from pdf.\")\n",
        "else:\n",
        "    print(f\"Failed to download PDF: {response.status_code}\")\n",
        "\n",
        "question = \"Qual é o impacto dos transformers na IA? Ha uma tendencia?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-UlzTf88Vwm"
      },
      "outputs": [],
      "source": [
        "client = ai.Client()\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question only based on the below text.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Answer the question based on the following text:\\n\\n{pdf_text}\\n\\nQuestion: {question}\\n\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ],
      "metadata": {
        "id": "fdLBaht28Vwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b741b75-b916-4700-a425-a9caf75d6c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PW7cJ-o8Vwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4e9897-a07f-43a7-d2e3-eb6e2f1c8736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O artigo \"Attention is All You Need\" descreve a influência dos Transformers na área de Inteligência Artificial (IA). Segundo o autor, os Transformers têm revolucionado a forma como os modelos de linguagem são treinados e desenvolvidos.\n",
            "\n",
            "Os Transformers são modelos de linguagem baseados em atenção que permitem que os modelos de linguagem acessem as diferentes partes de um entrada e combinem as informações da forma adequada. Eles são inspirados na forma como os humanos processam a linguagem, voltando e refletindo sobre diferentes partes de uma mensagem.\n",
            "\n",
            "O artigo argumenta que os Transformers têm três principais contribuições para a IA:\n",
            "\n",
            "1. **Aumento da eficiência**: Os Transformers permitem que os modelos de linguagem treinados mais rapidamente e com mais eficiência do que os modelos tradicionais.\n",
            "2. **Melhoria da precisação**: Os Transformers são capazes de capturar as nuances da linguagem de uma forma mais precisa do que os modelos tradicionais.\n",
            "3. **Ampliação das funções**: Os Transformers permitem que os modelos de linguagem sejam treinados para uma variedade de tarefas, incluindo classificação de texto, tradução automática e geração de texto.\n",
            "\n",
            "O autor também argumenta que a tendência é que os Transformers continue a ser uma ferramenta fundamental para o desenvolvimento da IA, especialmente nas áreas de linguagem e processamento de texto.\n",
            "\n",
            "Além disso, o artigo tem em vista a possibilidade de integração dos Transformers com outros conceitos da IA, como a memória e a motivação, a fim de criar modelos de linguagem mais avançados e capazes de imitar a linguagem humana.\n",
            "\n",
            "Em resumo, o impacto dos Transformers na IA é de aumentar a eficiência, melhorar a precisão e ampliar as funções dos modelos de linguagem, com uma tendência de continua evolução e integração com outros conceitos da IA.\n"
          ]
        }
      ],
      "source": [
        "anthropic_claude_3_opus = \"groq:llama-3.1-8b-instant\"\n",
        "response = client.chat.completions.create(model=anthropic_claude_3_opus, messages=messages)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['FIREWORKS_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ],
      "metadata": {
        "id": "rYrsOoWO8Vwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aadfY_Jc8Vwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b992d0-343d-4cb5-827f-6250439659d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os Transformers têm tido um impacto significativo na IA, revolucionando o campo da processamento de linguagem natural (PLN) e abrindo caminhos para aplicações mais complexas em áreas como rede de mensagens, tradução automática e análise de texto.\n",
            "\n",
            "Com a introdução dos Transformers em 2017, os pesquisadores conseguiram superar as limitações das redes convolucionais tradicionais em processar grandes quantidades de dados sequenciais, como textos ou séries temporais. Isso permitiu o desenvolvimento de modelos mais eficientes e precisos para tarefas como:\n",
            "\n",
            "1.  **Translação automática**: Os Models Transformer são capazes de realizar traduções precisas e rápidas entre línguas diferentes.\n",
            "2.  **Processamento de linguagem natural (PLN)**: Os modelos Transformer podem realizar tarefas como resumo automatizado, extração de entidades nominais e análise de sentimentos.\n",
            "3.  **Análise de texto**: Os modelos podem realizar tarefas como classificação automática, detecção de spam e recuperação de informações relevantes.\n",
            "\n",
            "A tendência atual é a evolução contínua desses modelos para se tornar ainda mais avançados e especializados em áreas específicas da IA.\n",
            "\n",
            "Além disso, a integração dos Transformers com outras tecnologias pode expandir suas capacidades, como:\n",
            "\n",
            "1.  **Inteligência artificial estreita (IAI)**: A combinação com IAI pode permitir que os modelos Transformer realizem tarefas mais complexas e específicas.\n",
            "2.  **Voz assistente inteligente**: A integração com sistemas speech recognition pode fazer dos modelo transformadores uma ferramenta poderosa no mundo do Voi Assistente Intelligente\n",
            "\n",
            "Em resumo, os Transformers têm sido uma inovação importante na IA e continuam a ser uma área ativa para investigação e desenvolvimento.\n"
          ]
        }
      ],
      "source": [
        "fireworks_model = \"fireworks:accounts/fireworks/models/llama-v3p2-3b-instruct\"\n",
        "response = client.chat.completions.create(model=fireworks_model, messages=messages, temperature=0.75, presence_penalty=0.5, frequency_penalty=0.5)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIM"
      ],
      "metadata": {
        "id": "j_2EQlEs8zz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML('<iframe width=\"380\" height=\"200\" src=\"https://www.youtube.com/embed/JDFTJgZ-FJ0\" title=\"LLM (Large Language Models) – Modelos de linguagem de larga escala e suas aplicações.\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "NFmxfGRBnVO8",
        "outputId": "9ab0bfda-0f71-4201-ef1e-02fa91d6559c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"380\" height=\"200\" src=\"https://www.youtube.com/embed/JDFTJgZ-FJ0\" title=\"LLM (Large Language Models) – Modelos de linguagem de larga escala e suas aplicações.\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}